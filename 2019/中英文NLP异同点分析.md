Chinese vs English

- 英语以表音 (字音) 构成, 汉语以表义 (字形) 构成
- 分词方式的不同
  - 现代汉语中表达意思的基本语素是词而不是字
  - 对中文进行自动语义分析时, 首要操作是分词 (Word Segmentation)
    - 机械切分 (正向/逆向最大匹配, 双向最大匹配等)
    - 统计切分 (HMM, CRF 等)
    - 深度神经网络切分 (RNN 等)
  - 中文分词常见的歧义类型
    - 交叉歧义 (Cross Ambiguity)
    - 组合歧义 (Combination Ambiguity)
- 英文词素与中文偏旁的使用
  - 英文特有的处理步骤包括:
    - 词形还原 (Lemmatization)
      - 英文单词有丰富的单复数, 主被动, 时态变换等 (共 16 种) 情况
      - 词形还原通常需要配合词形标注 (Pos-tag), 以确保还原准确度, 避免歧义
    - 词干提取 (Stemming)
      - 英文单词内部由若干词素构成, 可分为词根和词缀, 词根的原形称为词干.
      - 最常用的英文词素有 300+, 很多源自拉丁语和希腊语
      - 通过拆解词干, 能方便地把握单词含义.
      - 常见的 Stemming 方法:
        - Porter Stemming Algorithm
        - Lovins Algorithm
        - Lancaster (Paice/Husk) Algorithm
  - 中文的偏旁是与英文的词干相近的概念. (单词不懂看词干, 汉字不懂看偏旁)
  - 达观数据认为拆解汉字的偏旁部首效果不明显的原因: 常用的汉字数量比英文单词少得多 (这里有一个不对称的映射关系, 词语对单词, 又汉字对单词), 字少, 每个汉字的意思多, 汉字的含义通过上下文来获取语义描述信息足够充分, 拆分偏旁后额外能添补的信息作用不大. 另一个原因是, 汉字的演化, 使得字形和含义发生了巨变 (香侬科技的论文), 偏旁未必能准确表达字意.
- 词形标注方法的差异
  - 英文有一些中文没有的词性, 最典型的是冠词 (不定冠词, 定冠词, 零冠词) 和助动词.
    - 大量出现的冠词虽然是虚词, 本身没有明确含义, 但在NLP中用于定位关键实词, 判断实词类型 (是否可数, 是否专有名词等), 进而识别出句法结构等, 起到了很大的指示作用, 降低了计算机进行语义理解的难度
    - 助动词的作用是协助主要动词构成谓语词组, 常见的有: am, is, have, do, are, will, shall, would should, be going to 等. 和冠词用于指示主宾相似, 助动词对于识别主要动词 (main verb) 和谓语有帮助
  - 英文在词性方面的规划和使用更严谨, 词汇在变换词性的时候会在词尾形成丰富的变化 (-ing, -able, -ful, -ment, -ness 等)
    - 名词会进一步区分可数和不可数; 动词存在发生时态的指示.
    - 在英文语法中几乎不存在词性混淆不清的情况
  - 语言学家沈家煊在<语法六讲>中指出, 汉语动词和名词不分立.和英文中名词, 动词, 形容词相互独立的"分立模式"不同, 中文更类似"包含模式": 形容词作为一个次类包含在动词中, 动词本身又作为次类包含在名词中. 中文的语境变化给词性带来了非常微妙的变化.
  - 汉语一词多性, 但缺乏指示. 对于一些模糊的情况, 一些中文语料干脆标注成了"动名词 vn", "形名词 an"
- 标点符号和字体特征
  - 在实际工程应用中, 标点与字体等信息能提供帮助, 尤其是在辨识内容时起重要的引导作用.
  - 英语中, 句逗的使用和大小写的使用等都有明确规范, 都是良好的语义指示符, 有助于分割不同句子, 也在句子内部分割不同语义, 这为NLP处理创造了较好的环境.
  - 中文的句逗并非强制标准, 更偏向于指导意见.
  - 中文特有的标点符号: 书名号指示专有名词, 顿号指示并列关系; 英文则采用特殊字体来标识专有名词 (加粗, 斜体等, 但没有强制约定), 因此字体信息很重要
- 词汇粒度的处理方法差异
  - 词汇粒度与分词机制有很大的关系, 一般将最小粒度切分所得的词称为"基本粒度词".
  - 大粒度词的表义能力更强, 能完整准确地表达一个概念, 适合作为文章关键词或标签提取出来 (在搜索引擎中直接使用大粒度词构建倒排索引, 一般可得到相关性(准确率) 更好的结果, 但召回率会有所不足)
  - 一个成熟的分词器需要因地制宜地设置不同粒度的分词策略, 并且最好还能确保在检索词处理 (Query Analysis) 和索引构建 (Index Building) 两端的切分策略保持一致.
  - 中文语义分析时, 基本粒度是一个关键难题, 原因是现代汉语写作时, 既有现代双音节/多音节词汇, 也夹杂着古汉语的单字, 半文半白的现象很常见, 给中文 NLP 带来了挑战
- 句法结构分析方法异同
  - 句子结构由"主谓宾定状补"这样的句法元素构成
  - 在句子结构方面, 英语重形合, 中文重义合. 中文句子的重心往往后移, 相反英语句子的主要部分前移, 于是生成句法依存树时中文通常会自动选择靠后的名词
  - 英文为了充分体现句子中的各种承接/转折/从属/并列等关系, 不厌其烦地准备了大量连词/助动词/介词/冠词等作为填充剂, 来补充实词之间的缝隙, 构成了从句/引导句.
  - 中文的句子结构比较松散, 没有英文中大量的虚词作为实词间的语义粘合剂, 而是依赖词汇前后顺序关系, 隐含表达出句子结构
  - 中英文都会通过特定标识词连接子句间的关系 (转折/假设/递进/因果等), 使用时经常被自动忽略, 但英文一般省略其中一个, 而中文就可以全省掉, 也可以全写, 实际进行语义理解时需要额外补充处理.
  - 大部分 NLP 应用选择直接从词汇或者篇章级别来获得结果, 省去了中间句子一层.
- 中英文指代消解处理
  - 指代消解 (Reference resolution) 建立起上下文中词汇间的关联关系.
  - 指代消解通常又细分为回指, 预指, 共指等情形, 实践中通常被称为共指消解 (Coreference resolution)
  - 英文中常见指代语是专有名词首字母缩, 是表音文字的特别之处. 回指语是一个新的独立单词, 和原词汇的关联处理通过共指消解完成; 另一部分回指语是"it, which, where, there, that"等的指代词, 要通过上下文依赖关系去寻找实体.
  - 中文的缩写通常从实体中抽取若干汉字新构成的词. 由于汉字的单字表义能力比英文字母强得多, 在缩略语的可读性上优于英文.
  - 共指消解中会遇到一类因为语法结构导致的指向歧义问题.
  - 实际工程应用中, 共指消解最常用到的场景是对人名/机构名/地点/条款/具体事件/关系类型等要素的指代处理.
  - 日常中文的共指消解存在一定的行文规律, 通过预先挖掘简写和指代词导入算法可显著提升效果. 业界常见的共指消解方法既有传统的规则启发法, 也有经典的统计学习/聚类算法/概率模型等, 还有深度强化学习, LSTM 等新的 Neural Mention-ranking 方法.
- 中英文词汇关联关系挖掘
  - 词汇间关系是构建语义网络的一项基础技术, 常见的词汇关系包括: 同义词/近义词/相关词/上下位等.
  - 知识图谱是理解词汇间关系的一种好办法.
  - Word2Vec 的 embedding 技术弥补了英文中词汇间关系不直观的问题, 对提高计算机英文语义理解的能力起到了很好的帮助作用.
- 中英文省略和内容补足的处理
  - 人类在阅读时, 不止看到文字内容本身, 而是不自觉地会将语境相关的词汇自动补充进字里行间, 辅助语义理解. 反观人类的写作, 会将一些重复内容省略掉, 主宾是最常被省略掉的对象, 其次是一些连词.
  - 相较于中文, 英文书面文本中省略出现的情况较少, 语义连接词的省略有固定规范
- 歧义问题与子串转义处理
  - 中文特有的现象之一是, 子串转义, "周杰" 和 "周杰伦", 这也是分词的挑战之一.
  - 中文不是靠词汇的变形变换来体现修饰/主被动等关系, 而是靠顺序组合来体现. 在中文的各个环节, 从分词/词性/句法/指代/局部子串处理/都会带来歧义理解的问题
  - 英文也存在歧义问题, 最常见的情况是英文一词多义导致的.
  - 不同语言处理歧义的具体方法不同, 但整体思路都是将歧义句放到句子上下文中解读, 引入更多语境信息来正确获得意思.
  - 计算机进行语义理解, 某种程度上就是在和各种各样的歧义作斗争过程.
