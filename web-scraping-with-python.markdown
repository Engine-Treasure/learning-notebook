#Python网络数据采集

- 非原创即采集
- 一念清净, 烈焰成池, 一念觉醒, 方登彼岸
- 网络数据采集, 无非就是写一个自动化程序向网络服务器请求数据, 再对数据进行解析, 提取需要的信息
- 通常, 有api可用, api会比写网络爬虫程序来获取数据更加方便.

##Part1 创建爬虫

###Chapter1 初建网络爬虫

- 一旦你开始采集网络数据, 就会感受到浏览器为我们所做的所有细节, 它解释了所有的html, css, JavaScript
- 网络浏览器是一个非常有用的应用, 它创建信息的数据包, 发送, 并把获取的数据解释成图像, 声音, 视频, 或文字. 但网络浏览器就是代码, 而代码是可以分解的, 可以分解成许多基本组件, 可重写, 重用, 以及做成我们想要的任何东西
- "域名为kissg.me的服务器上<网络应用根地址>/pages目录下的html文件page1.html的源代码"
- 网络浏览器与爬虫程序的区别:
 - 浏览器遇到html标签时, 会向服务器再发起对该资源的请求, 再用请求得到的资源渲染页面
 - 爬虫程序并没有返回向服务器请求多个文件的逻辑, 它只能读取已经请求的单个html文件
- `BeautifulSoup`通过定位html标签来格式化和组织复杂的网络信息, 以python对象展示xml结构信息
- 先调用`response.read()`获取网页的内容, 再将html内容传给BeautifulSoup对象, 形成的结构如下所示:

```python
html → <html><head>...</head><body>...</body></html>
    — head → <head><title>A Useful Page<title></head>
        — title → <title>A Useful Page</title>
    — body → <body><h1>An Int...</h1><div>Lorem ip...</div></body>
        — h1 → <h1>An Interesting Title</h1>
        — div → <div>Lorem Ipsum dolor...</div>
```

- 因此可以通过`bsObj.html.body.h1`的方式提取标签
- 请求的网页在服务器上不存在时, 程序会返回http错误, urlopen函数会抛出`HTTPError`异常, 对此, 可使用`try ... except ...`语句来处理
- 服务器不错在, urlopen返回一个None对象, 可通过条件判断简单地处理
- 当调用的标签不存在时, BeautifulSoup会返回None对象, 请求None对象的子标签, 会发生`AttributeError`错误, 同样地, 也用`try ... except ... `语句来处理
- `一个良好的爬虫程序首先要具备强大而周密的异常处理能力, 随时应对网络中可能存在的异常`
- 在写爬虫的时候, 思考代码的总体格局, 让代码既可以**捕捉异常**又**容易阅读**, 很重要. 考虑代码的重用性

###Chapter2 复杂html解析

- 当米开朗基罗被问及如何完成"大卫"这样匠心独具的雕刻作品时, 他有一段著名的回答: "很简单, 你只要用锤子把石头上不像大卫的地方敲掉就行了." (大道至简)
- 解析html的前戏, 磨刀不勿砍柴功:
 1. 寻找"打印此页", 或看看网站有没有html样式更友好的移动版(将请求头设置成移动设备的状态, 然后接收网站移动版)
 2. 需找隐藏在js文件里的信息,
 3. 网页信息也许可以从网页的url链接中获取
 4. 寻找其他数据源, 有没有其他网站显示了同样的数据? 该网站的数据是不是来自其他网站
- css(cascading style sheet, 层叠样式表)为爬虫提供了便利, 它是html元素差异化, 使那些具有相同修饰的元素呈现不同的样式, 从而是爬虫的目标更明确
- `findAll()`的基础用法: `findAll(tag_name, dict_of_attributes)`
- 使用`bsObj.tagName`只能获取页面中第一个指定的标签
- `get_text()`方法正在处理的html文档中的所有标签都清楚掉, 返回只包含文字的字符串
- `findAll(tag, attributes, recursive, text, limit, **keywords)` & `find(tag, attributes, recursive, text, **keywords)`:
 - tag - 可以是单个标签名, 也可以是多个标签名组成的列表(集合)
 - attributes - 用字段封装的标签属性与属性值, 属性值可以是一个集合
 - recursive - 递归? 默认为True. 若recursive=False, findAll只会查文档的一级标签. 若对速度要求非常高, 可设置递归参数
 - text - 用标签的内容去匹配, 而不是标签名或属性,
 - limit - 限制findAll的搜索结果项
 - 关键字参数 - 允许用户自己指定属性的标签. 是BeautifulSoup在技术上做的冗余功能, 任何关键字参数能完成的任务, 都可以其他技术解决
- 通过标签参数`tag`把标签列表传到`.findAll()`里获取一列标签, 其实是一个"或"关系的过滤器. 而关键字参数可以增加一个"与"关系的过滤器来简化工作
- `Tag`对象 - `BeautifulSoup`对象通过`find`或`findAll`, 或直接调用子标签获取的一些列对象或单个对象
- `NavigableString`对象 - 用于表示标签里的文字
- `Comment`对象 - html文档的注释标签
- html页面可以映射成一棵树
- 子标签, 即一个父标签的下一级; 后代标签, 指一个父标签下面所有级别的标签. 所有子标签都是后代标签, 但不是所有的后代标签都是子标签
- 一般, bs函数总是处理当前标签的后代标签, 比如`.findAll`就是递归地在所有后代标签中查找
- 使用`.children`将只获取当前标签的子标签,  使用`descendants`将获取所有后代标签
- 使用`.next_siblings`将获得当前标签之后,所有的兄弟标签. 注意, 对象本身不能作为自己的兄弟标签; 从名字也可以看出, 是返回之后的兄弟标签. 相应的有`previous_siblings`,`next_sibling`和`previous_sibling`
- 使用`bsObj.find("table", {"id": "giftList"}).tr`而不是简单的`bsObj.table.tr`或`bsObj.tr`是为了让对象的选择更具体, 而不丢失标签的细节
- `parent`和`parents`分别用于获取标签对象的直接父标签和所有父辈标签(包括爷爷, 但不包括叔叔, 好吧, 通俗易懂)
- 在动手写正则表达式之前, 写一个步骤列表描述出目标字符串的结构
- regex:
 - `*` - 重复任意次, 包括0次
 - `|` - 表示或
 - `+` - 重复至少1次
 - `[]` - 匹配其中的任意一个字符
 - `()` - 表达式编组, 在regex的规则里编组会优先运行
 - `{m,n}` - 重复m到n次
 - `[^]` - 匹配一个任意不在方括号中的字符
 - `.` - 匹配任意单个字符
 - `^` - 标识字符串的开始
 - `\` - 转义字符
 - `$` - 标识字符串的结尾
- 在BeautifulSoup中使用regex, 提高效率, 如`images = bsObj.findAll("img",{"src":re.compile("\.\.imggifts/img.*\.jpg")})`
- regex可以作为BeautifulSoup语句的任意一个参数
- 对于标签对象, 可采用`.attrs`获取全部属性, 返回字典对象. `imgTag.attrs["src"]`就可以获取图片的资源位置
- BeautifulSoup允许将特定函数类型作为`findAll`函数的参数, 唯一的限制条件是这些函数必须将一个***标签作为参数***且***返回结果是布尔类型***. BeautifulSoup用这个函数评估遇到的每个标签对象, 将评估结果为"True"的标签保留下来, 将其他标签剔除. 如`soup.findAll(lambda tag: len(tag.attrs) == 2)`将返回具有2个属性的标签
- BeautifulSoup与regex与lambda的联合使用, 想想都无敌
- 其他的html解析模块:
 1. lxml - 底层, 大部分源代码用c写成, 因此处理速度会非常快
 2. HTML parser - 自带的

###Chapter3 开始采集

- 之所以叫网络爬虫, 是因为他们可以沿着网络爬行, 本质是一种*递归方式*: 为了找到url链接, 必须首先获取网页内容, 检查页面内容, 再寻找另一个url, 获取页面内容, 不断循环
- 使用网络爬虫的时候, 应谨慎地考虑需要消耗多少网络流量, 还要尽量思考能否让采集目标的服务器负载更低
- `维基百科六度分隔理论` - 任何2个不相干的词条, 都可以通过总数不超过6条的词条链接起来(包括原来的2个词条).
- 由此, 写爬虫时, 如何高效地通过最少的链接点击次数到达目的站点, 不仅使爬虫工作效率更高, 且对服务器的负载影响也越小
- 爬取的内容往往携带许多无用的信息, 在处理之前, 应根据实际剔除无用信息
- 随机算法都努力创造一种均匀分布且难以预测的数据序列, 但在算法初始阶段都需要提供一个随机数种子(random seed). 完全相同的种子每次将产生相同的"随机"数序列. 可以用系统当前时间作为随机数种子, 而使程序运行更具随机性.
- python的伪随机数生成器用的是梅森旋转算法(https://en.wikipedia.org/wiki/Mersenne_Twister)
- 浅网(surface web)是搜索引擎可以抓取的网络; 暗网(dark web)或深网(deep web)则是另一部分. 据不完全统计,互联网中其实约 90% 的网络都是深网.
- 暗网,也被称为 Darknet 或 dark Internet,完全是另一种“怪兽”。它们也建立在已有的网络基础上,但是使用 Tor 客户端,带有运行在 HTTP 之上的新协议,提供了一个信息交换的安全隧道。这类暗网页面也是可以采集的。
- 遍历整个网站的数据采集的好处:
 1. 生成网站地图(脉络)
 2. 收集数据
- 为了避免重复采集页面, 使用`set`来保存已采集的页面
- 如果递归运行的次数过多, 递归程序可能会崩溃. python的默认递归限制是1000次.
- 在开始写爬虫程序之前, 应充分分析待爬取网站的html文档的格式
- 在一个异常处理语句中包裹多行语句显然是有点危险的. 首先无法识别出究竟哪行代码出现了异常, 其次前面的语句出现异常, 将直接导致后面语句的执行
- 网络爬虫位于许多新式的网络技术领域彼此交叉的中心地带. 要实现跨站的数据分析, 只要构建出可以从互联网上的网页里解析和存储数据的爬虫就可以了
- 一个网站内部的爬虫, 只需要爬取以`/`开始的资源就可以了
- 在开始写爬虫跟随外链随意跳转之前, 该思考的问题:
 - 我要收集哪些数据?这些数据可以通过采集几个已经确定的网站(永远是最简单的做法)完成吗?或者我的爬虫需要发现那些我可能不知道的网站吗?
 - 当我的爬虫到了某个网站,它是立即顺着下一个出站链接跳到一个新网站,还是在网站上呆一会儿,深入采集网站的内容?
 - 有没有我不想采集的一类网站?我对非英文网站的内容感兴趣吗?
 - 如果我的网络爬虫引起了某个网站网管的怀疑,我如何避免法律责任?
- 在以任何正式目的运行代码之前, 确保已经在可能出现问题的地方都放置了检查语句
- 写代码之前拟个大纲或画个流程图, 是一个很好的编程习惯, 不仅可以为后期处理节省时间, 更重要的是可以防止自己在爬虫变得越来越复杂时乱了分寸
- `重定向` - 允许一个网页在不同的域名下显示, 有2种形式:
 1. 服务器端重定向, 网页在加载之前先改变了url
 2. 客户端重定向, 跳转到新url之前网页需要加载内容
- python3版本的urllib库会自动处理重定向. 不过要注意,有时候要采集的页面的 URL 可能并不是当前所在页面的url
- 爬虫的一些基本模式: 找出页面上的所有链接, 区分内外链, 跳转到新的页面


###Chapter4 使用API

- API的用处:为不同的应用提供了方便友好的接口。不同的开发者用不同的架构,甚至不同的语言编写软件都没问题——因为API设计的目的就是要成为一种通用语言,让不同的软件进行信息共享。
- API 可以通过 HTTP 协议下载文件,和 URL 访问网站获取数据的协议一样,它几乎可以实现所有在网上干的事情。API 之所以叫 API 而不是叫网站的原因,其实是首先 API 请求使用非常严谨的语法,其次 API 用 JSON 或 XML 格式表示数据,而不是HTML 格式。
- 通常api的验证方法都是用类似令牌(token)的方式调用, 每次api调用将令牌传递给服务器. token除了在url链接中传递, 还会通过请求头里的cookie将用户信息传递给服务器:

```
token = token
webRequest = urllib.request.Request("http://xxx", headers={"token": token})
```

- api一个重要的特征是反馈格式友好的数据, xml或json. 目前json比xml更受欢迎, 因为json文件比完整的xml文件小, 另一个原因是网络技术的改变.
- 使用get请求获取数据时, 用url路径描述获取的数据范围, 查询参数可以作为过滤器或附加请求使用.
- 一些api使用文件路径形式指定api版本, 数据和其他属性:

```text
http://socialmediasite.com/api/v4/json/users/1234/posts?from=08012014&to=08312014
```

- 一些api通过请求参数的形式指定数据格式和api版本:

```text
http://socialmediasite.com/users/1234/posts?format=json&from=08012014&to=08312014
```

- `response.read()` -> bytes
- python将json转换成字典, json数组转换成列表, json字符串转换成pyton字符串
- 如果你用API作为唯一的数据源,那么你最多就是复制别人数据库里的数据,不过都是些已经公布过的“黄花菜”。真正有意思的事情,是把多个数据源组合成新的形式,或者把 API 作为一种工具,从全新的视角对采集到的数据进行解释
- 虽然列表迭代速度更快, 但集合查找速度更快(确定一个对象是否在集合中).
- python的集合就是值为None的字典, 用到是hash表结构, 查询速度为O(1)
- 多种技术的融合, 多种数据的融合, 将得到更有用的信息

###Chapter5 存储数据

- 大数据存储与数据交互能力, 在新式的程序开发中已经是重中之重了.
- 存储媒体文件的2种主要方式: 只获取url链接, 或直接将源文件下载下来
- 直接引用url链接的优点:
 - 爬虫运行得更快,耗费的流量更少,因为只要链接,不需要下载文件。
 - 可以节省很多存储空间,因为只需要存储 URL 链接就可以。
 - 存储 URL 的代码更容易写,也不需要实现文件下载代码。
 - 不下载文件能够降低目标主机服务器的负载。
- 直接引用url链接的缺点:
 - 这些内嵌在网站或应用中的外站 URL 链接被称为盗链(hotlinking), 每个网站都会实施防盗链措施。
 - 因为链接文件在别人的服务器上,所以应用就要跟着别人的节奏运行了。
 - 盗链是很容易改变的。如果盗链图片放在博客上,要是被对方服务器发现,很可能被恶搞。如果 URL 链接存起来准备以后再用,可能用的时候链接已经失效了,或者是变成了完全无关的内容。
- python3的urllib.request.`urlretrieve`可以根据文件的url下载文件:

```python
from urllib.request import urlretrieve
from urllib.request import urlopen
from bs4 import BeautifulSoup
html = urlopen("http://www.pythonscraping.com")
bsObj = BeautifulSoup(html)
imageLocation = bsObj.find("a", {"id": "logo"}).find("img")["src"]
urlretrieve (imageLocation, "logo.jpg")
```

- csv(comma-separated values, 逗号分隔值)是存储表格数据的常用文件格式
- 网络数据采集的一个常用功能就是获取html表格并写入csv
- 除了用户定义的变量名,mysql是不区分大小写的, 习惯上mysql关键字用大写表示
- 连接与游标(connection/cursor)是数据库编程的2种模式:
 - 连接模式除了要连接数据库之外, 还要发送数据库信息, 处理回滚操作, 创建游标对象等
 - 一个连接可以创建多个游标, 一个游标跟踪一种状态信息, 比如数据库的使用状态. 游标还会包含最后一次查询执行的结果. 通过调用游标函数, 如`fetchall`获取查询结果
 - 游标与连接使用完毕之后,务必要关闭, 否则会导致连接泄漏, 会一直消耗数据库资源
- 使用`try ... finally`语句保证数据库连接与游标的关闭
- 让数据库更高效的几种方法:
 1. 给每张表都增加id字段. 通常数据库很难智能地选择主键
 2. 用智能索引, `CREATE INDEX definition ON dictionary (id, definition(16));`
 3. 选择合适的范式
- 发送Email, 通过爬虫或api获取信息, 设置条件自动发送Email! 那些订阅邮件, 肯定就是这么来的!

###Chapter6 读取文档

- 互联网的最基本特征: 作为不同类型文件的传输媒体
- 互联网不是一个html页面的集合, 它是一个信息的集合, 而html文件只是展示信息的一个框架
- 从最底层的角度看, 所有文档都是01的编码, 而任意类型的文件的唯一区别就在于,它们的01在面向用户的转换方式不同
- `utf-8`的全称: Universal Character Set - Transformation Format 8 bit
- utf-8的每个字符开头有一个标记,表示该字符用几个字节表示.一个字符最多可以是4字节, 但字节信息里还包括一部分设置信息, 因此全部32位不会都用, 最多使用21位
- utf8利用ascii的填充位让所有以"0"开头的字节表示该字符占用1个字节. 因此, 在英文字符在ascii和uft8两个编码方式下表示一样
- python默认将文本读成ascii编码格式
- utf8不能处理iso编码格式. 因此做数据采集工作时,尤其对国际网咱, 最好先看看meta标签内容, 用网站推荐的编码方式读取页面内容
- 几种读取在线文件的方法:
 1. 下载读取
 2. 从网上直接将文件读成一个字符串, 然后将其转换成一个StringIO对象, 使其具有文件的属性:

```python
dataFile = io.StringIO(data)
```

- `csv.reader`的返回对象是可迭代的, 而且由list对象构成.
- `csv.DictReader`将csv文件的每一行转换成python字典返回, 并将字段列表(标题栏)保存在dictReader.fieldnames里
- pdf的文字提取
- .docx的文字提取：
 1. 从文件读取xml, 将文档读成一个二进制对象(BytesIO), 再用zipfile解压(所有.docx文件为了节省空间都进行过压缩), 再读取解压文件, 就变成了xml

```python
from zipfile import ZipFile
from urllib.request import urlopen
from io import BytesIO

wordFile = urlopen("http://pythonscraping.com/pages/AWordDocument.docx").read()
wordFile = BytesIO(wordFile)
document = ZipFile(wordFile)
xml_content = document.read("word/document.xml")
print(xml_content.decode("utf-8"))
```

##Part2 高阶数据采集

- 网站的真实故事其实都隐藏在js, 登录表单和网站反爬取措施的背后

###Chapter7 数据清洗

- 用regex移除不想要的字符, 如换行符(\n). 剔除字符的过程, 合理的先后顺序能省很多力
- 用***先以utf8格式编码,再以ascii方法解码***的方式,可以一定程度上剔除unicode字符
- 数据标准化要确保清洗后的数据在语言学或逻辑上是等价的
- python的`OrderedDict`,能解决字典无序排列的问题
- 数据标准化时,根据投入计算能力的多少,还可以再考虑大小写(python与Python),单词等价(1st与first),连字符的使用(co-ordinated与coordinated),拼写错误,语病等因素
- 对连字符的一个处理是,将其去掉或转换成其他字符,比如空格

###Chapter8 自然语言处理

- n-gram模型可用于词频分析, 很厉害!
- `马尔可夫文字生成器(markov text generator)` - 基于一种常用于分析大量随机事件的马尔可夫模型. 随机事件的特点是一个离散事件发生之后, 另一个离散事件将在前一个事件的条件下以一定概率发生
- 在马尔可夫模型描述的天气系统中,如果今天是晴天,那么明天有70%的可能是晴天,20%的可能多云,10% 的可能下雨。如果今天是下雨天,那么明天有 50% 的可能也下雨,25% 的可能是晴天,25% 的可能是多云
- 马尔可夫模型需要注意的点:
 - 任何一个节点引出的所有可能的总和必须等于 100%。无论是多么复杂的系统,必然会在下一步发生若干事件中的一个事件。
 - 只有当前节点的状态会影响下一个状态。
 - 有些节点可能比其他节点较难到达
- google的pagerank算法也是基于马尔可夫模型的, 将网站看作节点, 入站/出站链接作为节点的连线. 连接某个节点的可能性(linklihood)表示一个网站的相对关注度
- 马尔可夫文字生成器的工作原理: 对文献中的每一个单词进行有效处理, 再建立一个二维字典, 用于统计二元词组的词频. 每次以当前单词所在节点为查询表, 选择下一个节点. 随机生成一个权重, 用词频减权重, 一旦权重减为非正数, 确定该单词为下一单词. 词频高的单词使权重减小得更厉害, 因此更容易获得
- 在寻找有向图的最短路径问题中, 效果最好且最常用的方法是`广度优先搜索(breadth-first search, bfs)`

