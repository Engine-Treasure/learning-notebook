#Python网络数据采集

- 非原创即采集
- 一念清净, 烈焰成池, 一念觉醒, 方登彼岸
- 网络数据采集, 无非就是写一个自动化程序向网络服务器请求数据, 再对数据进行解析, 提取需要的信息
- 通常, 有api可用, api会比写网络爬虫程序来获取数据更加方便.

##Part1 创建爬虫

###Chapter1 初建网络爬虫

- 一旦你开始采集网络数据, 就会感受到浏览器为我们所做的所有细节, 它解释了所有的html, css, JavaScript
- 网络浏览器是一个非常有用的应用, 它创建信息的数据包, 发送, 并把获取的数据解释成图像, 声音, 视频, 或文字. 但网络浏览器就是代码, 而代码是可以分解的, 可以分解成许多基本组件, 可重写, 重用, 以及做成我们想要的任何东西
- "域名为kissg.me的服务器上<网络应用根地址>/pages目录下的html文件page1.html的源代码"
- 网络浏览器与爬虫程序的区别:
 - 浏览器遇到html标签时, 会向服务器再发起对该资源的请求, 再用请求得到的资源渲染页面
 - 爬虫程序并没有返回向服务器请求多个文件的逻辑, 它只能读取已经请求的单个html文件
- `BeautifulSoup`通过定位html标签来格式化和组织复杂的网络信息, 以python对象展示xml结构信息
- 先调用`response.read()`获取网页的内容, 再将html内容传给BeautifulSoup对象, 形成的结构如下所示:

```python
html → <html><head>...</head><body>...</body></html>
    — head → <head><title>A Useful Page<title></head>
        — title → <title>A Useful Page</title>
    — body → <body><h1>An Int...</h1><div>Lorem ip...</div></body>
        — h1 → <h1>An Interesting Title</h1>
        — div → <div>Lorem Ipsum dolor...</div>
```

- 因此可以通过`bsObj.html.body.h1`的方式提取标签
- 请求的网页在服务器上不存在时, 程序会返回http错误, urlopen函数会抛出`HTTPError`异常, 对此, 可使用`try ... except ...`语句来处理
- 服务器不错在, urlopen返回一个None对象, 可通过条件判断简单地处理
- 当调用的标签不存在时, BeautifulSoup会返回None对象, 请求None对象的子标签, 会发生`AttributeError`错误, 同样地, 也用`try ... except ... `语句来处理
- `一个良好的爬虫程序首先要具备强大而周密的异常处理能力, 随时应对网络中可能存在的异常`
- 在写爬虫的时候, 思考代码的总体格局, 让代码既可以**捕捉异常**又**容易阅读**, 很重要. 考虑代码的重用性

###Chapter2 复杂html解析

- 当米开朗基罗被问及如何完成"大卫"这样匠心独具的雕刻作品时, 他有一段著名的回答: "很简单, 你只要用锤子把石头上不像大卫的地方敲掉就行了." (大道至简)
- 解析html的前戏, 磨刀不勿砍柴功:
 1. 寻找"打印此页", 或看看网站有没有html样式更友好的移动版(将请求头设置成移动设备的状态, 然后接收网站移动版)
 2. 需找隐藏在js文件里的信息,
 3. 网页信息也许可以从网页的url链接中获取
 4. 寻找其他数据源, 有没有其他网站显示了同样的数据? 该网站的数据是不是来自其他网站
- css(cascading style sheet, 层叠样式表)为爬虫提供了便利, 它是html元素差异化, 使那些具有相同修饰的元素呈现不同的样式, 从而是爬虫的目标更明确
- `findAll()`的基础用法: `findAll(tag_name, dict_of_attributes)`
- 使用`bsObj.tagName`只能获取页面中第一个指定的标签
- `get_text()`方法正在处理的html文档中的所有标签都清楚掉, 返回只包含文字的字符串
- `findAll(tag, attributes, recursive, text, limit, **keywords)` & `find(tag, attributes, recursive, text, **keywords)`:
 - tag - 可以是单个标签名, 也可以是多个标签名组成的列表(集合)
 - attributes - 用字段封装的标签属性与属性值, 属性值可以是一个集合
 - recursive - 递归? 默认为True. 若recursive=False, findAll只会查文档的一级标签. 若对速度要求非常高, 可设置递归参数
 - text - 用标签的内容去匹配, 而不是标签名或属性,
 - limit - 限制findAll的搜索结果项
 - 关键字参数 - 允许用户自己指定属性的标签. 是BeautifulSoup在技术上做的冗余功能, 任何关键字参数能完成的任务, 都可以其他技术解决
- 通过标签参数`tag`把标签列表传到`.findAll()`里获取一列标签, 其实是一个"或"关系的过滤器. 而关键字参数可以增加一个"与"关系的过滤器来简化工作
- `Tag`对象 - `BeautifulSoup`对象通过`find`或`findAll`, 或直接调用子标签获取的一些列对象或单个对象
- `NavigableString`对象 - 用于表示标签里的文字
- `Comment`对象 - html文档的注释标签
- html页面可以映射成一棵树
- 子标签, 即一个父标签的下一级; 后代标签, 指一个父标签下面所有级别的标签. 所有子标签都是后代标签, 但不是所有的后代标签都是子标签
- 一般, bs函数总是处理当前标签的后代标签, 比如`.findAll`就是递归地在所有后代标签中查找
- 使用`.children`将只获取当前标签的子标签,  使用`descendants`将获取所有后代标签
- 使用`.next_siblings`将获得当前标签之后,所有的兄弟标签. 注意, 对象本身不能作为自己的兄弟标签; 从名字也可以看出, 是返回之后的兄弟标签. 相应的有`previous_siblings`,`next_sibling`和`previous_sibling`
- 使用`bsObj.find("table", {"id": "giftList"}).tr`而不是简单的`bsObj.table.tr`或`bsObj.tr`是为了让对象的选择更具体, 而不丢失标签的细节
- `parent`和`parents`分别用于获取标签对象的直接父标签和所有父辈标签(包括爷爷, 但不包括叔叔, 好吧, 通俗易懂)
- 在动手写正则表达式之前, 写一个步骤列表描述出目标字符串的结构
- regex:
 - `*` - 重复任意次, 包括0次
 - `|` - 表示或
 - `+` - 重复至少1次
 - `[]` - 匹配其中的任意一个字符
 - `()` - 表达式编组, 在regex的规则里编组会优先运行
 - `{m,n}` - 重复m到n次
 - `[^]` - 匹配一个任意不在方括号中的字符
 - `.` - 匹配任意单个字符
 - `^` - 标识字符串的开始
 - `\` - 转义字符
 - `$` - 标识字符串的结尾
- 在BeautifulSoup中使用regex, 提高效率, 如`images = bsObj.findAll("img",{"src":re.compile("\.\.imggifts/img.*\.jpg")})`
- regex可以作为BeautifulSoup语句的任意一个参数
- 对于标签对象, 可采用`.attrs`获取全部属性, 返回字典对象. `imgTag.attrs["src"]`就可以获取图片的资源位置
- BeautifulSoup允许将特定函数类型作为`findAll`函数的参数, 唯一的限制条件是这些函数必须将一个***标签作为参数***且***返回结果是布尔类型***. BeautifulSoup用这个函数评估遇到的每个标签对象, 将评估结果为"True"的标签保留下来, 将其他标签剔除. 如`soup.findAll(lambda tag: len(tag.attrs) == 2)`将返回具有2个属性的标签
- BeautifulSoup与regex与lambda的联合使用, 想想都无敌
- 其他的html解析模块:
 1. lxml - 底层, 大部分源代码用c写成, 因此处理速度会非常快
 2. HTML parser - 自带的

###Chapter3 开始采集

- 之所以叫网络爬虫, 是因为他们可以沿着网络爬行, 本质是一种*递归方式*: 为了找到url链接, 必须首先获取网页内容, 检查页面内容, 再寻找另一个url, 获取页面内容, 不断循环
- 使用网络爬虫的时候, 应谨慎地考虑需要消耗多少网络流量, 还要尽量思考能否让采集目标的服务器负载更低
- `维基百科六度分隔理论` - 任何2个不相干的词条, 都可以通过总数不超过6条的词条链接起来(包括原来的2个词条).
- 由此, 写爬虫时, 如何高效地通过最少的链接点击次数到达目的站点, 不仅使爬虫工作效率更高, 且对服务器的负载影响也越小
- 爬取的内容往往携带许多无用的信息, 在处理之前, 应根据实际剔除无用信息
- 随机算法都努力创造一种均匀分布且难以预测的数据序列, 但在算法初始阶段都需要提供一个随机数种子(random seed). 完全相同的种子每次将产生相同的"随机"数序列. 可以用系统当前时间作为随机数种子, 而使程序运行更具随机性.
- python的伪随机数生成器用的是梅森旋转算法(https://en.wikipedia.org/wiki/Mersenne_Twister)
- 浅网(surface web)是搜索引擎可以抓取的网络; 暗网(dark web)或深网(deep web)则是另一部分. 据不完全统计,互联网中其实约 90% 的网络都是深网.
- 暗网,也被称为 Darknet 或 dark Internet,完全是另一种“怪兽”。它们也建立在已有的网络基础上,但是使用 Tor 客户端,带有运行在 HTTP 之上的新协议,提供了一个信息交换的安全隧道。这类暗网页面也是可以采集的。
- 遍历整个网站的数据采集的好处:
 1. 生成网站地图(脉络)
 2. 收集数据
- 为了避免重复采集页面, 使用`set`来保存已采集的页面
- 如果递归运行的次数过多, 递归程序可能会崩溃. python的默认递归限制是1000次.
- 在开始写爬虫程序之前, 应充分分析待爬取网站的html文档的格式
- 在一个异常处理语句中包裹多行语句显然是有点危险的. 首先无法识别出究竟哪行代码出现了异常, 其次前面的语句出现异常, 将直接导致后面语句的执行
- 网络爬虫位于许多新式的网络技术领域彼此交叉的中心地带. 要实现跨站的数据分析, 只要构建出可以从互联网上的网页里解析和存储数据的爬虫就可以了
- 一个网站内部的爬虫, 只需要爬取以`/`开始的资源就可以了
- 在开始写爬虫跟随外链随意跳转之前, 该思考的问题:
 - 我要收集哪些数据?这些数据可以通过采集几个已经确定的网站(永远是最简单的做法)完成吗?或者我的爬虫需要发现那些我可能不知道的网站吗?
 - 当我的爬虫到了某个网站,它是立即顺着下一个出站链接跳到一个新网站,还是在网站上呆一会儿,深入采集网站的内容?
 - 有没有我不想采集的一类网站?我对非英文网站的内容感兴趣吗?
 - 如果我的网络爬虫引起了某个网站网管的怀疑,我如何避免法律责任?
- 在以任何正式目的运行代码之前, 确保已经在可能出现问题的地方都放置了检查语句
- 写代码之前拟个大纲或画个流程图, 是一个很好的编程习惯, 不仅可以为后期处理节省时间, 更重要的是可以防止自己在爬虫变得越来越复杂时乱了分寸
- `重定向` - 允许一个网页在不同的域名下显示, 有2种形式:
 1. 服务器端重定向, 网页在加载之前先改变了url
 2. 客户端重定向, 跳转到新url之前网页需要加载内容
- python3版本的urllib库会自动处理重定向. 不过要注意,有时候要采集的页面的 URL 可能并不是当前所在页面的url
- 爬虫的一些基本模式: 找出页面上的所有链接, 区分内外链, 跳转到新的页面


###Chapter4 使用API

- API的用处:为不同的应用提供了方便友好的接口。不同的开发者用不同的架构,甚至不同的语言编写软件都没问题——因为API设计的目的就是要成为一种通用语言,让不同的软件进行信息共享。
- API 可以通过 HTTP 协议下载文件,和 URL 访问网站获取数据的协议一样,它几乎可以实现所有在网上干的事情。API 之所以叫 API 而不是叫网站的原因,其实是首先 API 请求使用非常严谨的语法,其次 API 用 JSON 或 XML 格式表示数据,而不是HTML 格式。
- 通常api的验证方法都是用类似令牌(token)的方式调用, 每次api调用将令牌传递给服务器. token除了在url链接中传递, 还会通过请求头里的cookie将用户信息传递给服务器:

```
token = token
webRequest = urllib.request.Request("http://xxx", headers={"token": token})
```

- api一个重要的特征是反馈格式友好的数据, xml或json. 目前json比xml更受欢迎, 因为json文件比完整的xml文件小, 另一个原因是网络技术的改变.
- 使用get请求获取数据时, 用url路径描述获取的数据范围, 查询参数可以作为过滤器或附加请求使用.
- 一些api使用文件路径形式指定api版本, 数据和其他属性:

```text
http://socialmediasite.com/api/v4/json/users/1234/posts?from=08012014&to=08312014
```

- 一个api通过请求参数的形式指定数据格式和api版本:

```text
http://socialmediasite.com/users/1234/posts?format=json&from=08012014&to=08312014
```
.
- 一些api使用文件路径形式指定api版本, 数据和其他属性:

```text
http://socialmediasite.com/api/v4/json/users/1234/posts?from=08012014&to=08312014
```

- 一个api通过请求参数的形式指定数据格式和api版本:

```text
```

- `response.read()` -> bytes
- python将json转换成字典, json数组转换成列表, json字符串转换成pyton字符串
- 如果你用API作为唯一的数据源,那么你最多就是复制别人数据库里的数据,不过都是些已经公布过的“黄花菜”。真正有意思的事情,是把多个数据源组合成新的形式,或者把 API 作为一种工具,从全新的视角对采集到的数据进行解释
- 虽然列表迭代速度更快, 但集合查找速度更快(确定一个对象是否在集合中).
- python的集合就是值为None的字典, 用到是hash表结构, 查询速度为O(1)
-   
